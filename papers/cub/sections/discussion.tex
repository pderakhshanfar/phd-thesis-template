
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:cub:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------
\subsection{Execution weights}
%------------------------------------------

In our evaluation, we collected execution weights using an instrumented version of \jabref distributed to five different users. As a result, a large number of log messages allowed us to have execution weights for many different classes. Such data collection is not realistic in an industrial setting as the collection and analysis of log data is challenging for large applications \cite{Candido2019}. It is likely that the collected data will not cover the complete system but only a subset of its classes. 

However, we believe that the development of scalable \emph{software analytics} practices \cite{Menzies2013} represents an opportunity to include information from the software operations environment in various development activities, including testing \cite{Candido2019, Chen2018d}. For instance, Winter \etal \cite{Winter2019} recently brought information about the number of times a log statement is executed to the developer's IDE to raise awareness about the load it represented during the operations of the system. Similar information can be collected for seeding (like Chapter \ref{sec:model_seeding}) or annotating a control flow graph, like in our approach, and allow developers to generate new tests from their IDE.

Finally, our current approach considers execution weights individually to approximate usages, which allows us to compute a commonality score quickly. Different definitions of the commonality score based, for instance, on full and partial execution paths identified from the operations logs are possible. Those finer grained definitions of commonality would allow to take multiple executions of the same code blocks (like loops) into account, at the expense of a higher computational cost. Exploration and evaluation of such definitions are left for future work. 

% fact that execution weights are considered independently (same kind of discussion as 2-gram inference)

%------------------------------------------
\subsection{Impact on mutation analysis}
%------------------------------------------

After manual investigation of the generated tests\footnote{The results and complete manual analysis are available online \cite{Evers2020, evers_bjorn_2020_3897513, evers_bjorn_2020_3894711}.}, we saw that the classes on which \com performs relatively well are often executed. The class on which the performance was especially good compared to \df was an enum class (\texttt{org.jab\-ref.lo\-gic.util.Stan\-dard\-File\-Type}).
%
In a large majority of the cases (25 out of 30), the tests generated using the \com secondary objective contain a method sequence that is not present in the tests generated by \df.
%From inspection of the execution counts that have been gathered, it turns out that there is only one branch, the single branch of a method, that has been executed by the data collection participants. 
We inspected the execution counts of individual branches stemming from the operational use of the system. From this inspection, we found a single branch in the code of method \texttt{getExtensions} that has been executed as part of the usage scenarios from all participants. 

This method is consistently involved in the test cases that kill the mutants that the tests generated by \df fail to kill most of the time. 
This supports our initial assumption that using \com can drive the search process to cover the code in a different way, possibly finding different kinds of faults.

%------------------------------------------
\subsection{Usefulness for debugging}
%------------------------------------------

The end-goal of any test suite is to identify faults in source code and help the developer during her debugging activities. Our evaluation measured the fault-finding capabilities of the generated test suites, but did not investigate their usefulness for debugging. Previous research has confirmed that automatically generated tests can find faults in real software~\cite{almasi2017industrial} and are useful for debugging~\cite{Ceccato2015}. 

However, there remain several challenges, like the understandability of the generated tests \cite{FA13challenges, almasi2017industrial}. Since the \com and \ucom secondary objectives aim to influence how the lines in a method are covered, we expect that it will also have an impact on the understandability of the generated tests. Future research will include the assessment of the debugging capabilities of the generated tests (\eg understandability, performance, readability, \etc).


%------------------------------------------
\subsection{Threats to the validity}
%------------------------------------------

\paragraph{Internal validity}

We repeated each execution of \evosuite 30 times to take the randomness of the generation process into account in our data analysis. 
We have tested our implementation of the \com and \ucom secondary objectives to reduce bugs' chance of influencing our results. 

\paragraph{External validity}

We gathered execution data needed from a small number of people in a relatively structured manner. We cannot guarantee that those executions are representative of all the usages of \jabref. However, we believe that the diversity of the tasks performed by our users is enough for this evaluation. 
Also, the evaluation was performed using only one case study. Future research includes the repetition of the assessment on other Java applications. 

\paragraph{Construct validity}

We relied on the reports produced by \evosuite for structural coverage and the reports produced by \pit for mutation analysis to compare our different secondary objectives. The usage of those standard metrics allows the comparison of our results with other search-based unit test generation approaches. 

\paragraph{Conclusion validity}

Our conclusions were only drawn based on statistically significant results with $\alpha=0.05$. We used the standard non-parametric Wilcoxon Rank Sum test for significance and the Vargha-Delaney statistic for the effect size. 
