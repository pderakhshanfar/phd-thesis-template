Our evaluation aims to answer three research questions.
The first research question analyzes the levels of CBC coverage that can be achieved by \cling. 
% \begin{itemize}
% 	\item[\textbf{RQ1}] \textit{To what extent is \cling able to achieve high Coupled Branch Coverage?}
% \end{itemize}
For this research question, we first analyze the coupled branches covered by \cling in each of the cases:
\begin{itemize}
		\item[\textbf{RQ1.1}] \textit{What is the CBC coverage achieved by \cling? }
\end{itemize}
We also draw a comparison between \cling and \evosuite in terms of CBC coverage:
\begin{itemize}
		\item[\textbf{RQ1.2}] \textit{Can \cling cover more coupled branches compared to unit-level tests? }
\end{itemize}
Since the test cases generated by \cling aim to cover coupled branches between two classes, we need to determine the effectiveness of this kind of coverage compared to test suites generated for high branch coverage in unit testing:
\begin{itemize}
	\item[\textbf{RQ2}] \textit{What is the effectiveness of the integration-level tests compared to unit-level tests?}
\end{itemize} 
Finally, we want to see whether the tests generated by \cling can make any difference in practice. Hence, we analyzed the integration faults captured by these tests:
\begin{itemize}
	\item[\textbf{RQ3}] \textit{What integration faults does \cling detect?}
\end{itemize}

\subsection{Implementation}
\label{sec:implementation}

We implemented \cling as an open-source tool\footnote{\url{https://github.com/STAMP-project/botsing/tree/master/cling}} written in Java. The tool implements the code instrumentation for pairs of classes, builds the CCFGs at the byte-code level, and derives the coverage targets (pairs of branches) according to the CBC criterion introduced in Section~\ref{sec:approach:coupledBranchCrit}. The tool also implements the search heuristics, which are applied to compute the objective scores as described in Section~\ref{sec:cling:approach}. 
% For the search algorithms, \cling re-uses the algorithms available in \evosuite \cite{Fraser2011}, which is an external maven dependency.
 Besides, \cling implements the repair procedure described in Section~\ref{sec:mutation_and_crossover}, which extends the interface of the genetic operators in \evosuite. 
 Moreover, we customized the many-objective MOSA algorithm~\cite{Panichella2018}, which is implemented in \evosuite, for our test case generation problem in \cling.
%  This allows us to re-use the original implementation of state-of-the-art search algorithms (e.g., WS~\cite{Campos2017}, DynaMOSA~\cite{Panichella2018}, and MIO~\cite{Arcuri2019}) and customize it 


%
%We evaluate the complementarity of \integration (\emph{C.}) with search based unit test case generation. We used \evosuite (\emph{E.})  and the DybaMOSA algorithm \cite{Panichella2018}, as it is the one of the bests unit test generation tool available \cite{Panichella2018a}, to generate unit tests.
%Since \integration only considering branch pairs coverage, we set the branches coverage of the class under test as the goals of  \evosuite.

\subsubsection{Baseline Selection}

The goal of this evaluation is to explore the impact of the tests generated by \integration on the results of the search-based unit testing in various aspects. To achieve this purpose, we run our tool against \evosuite, which is currently the best tool in terms of achieving branch coverage \cite{rueda2016unit, panichella2017java, molina2018java, kifetew2019java}. Among the genetic algorithms which are implemented and evaluated in this tool, we choose \emph{DynaMosa}, which has the best outcome in structural and mutation coverage \cite{Panichella2018}. 
%This algorithm can be applied for various goals such as line coverage, branch coverage, weak mutation, \etc 

\subsection{Study Setup}
\label{sec:cling:setup}

\subsubsection{Subjects Selection}

\begin{table*} [t]
	\center
	\caption{Projects in our empirical study. \texttt{\#} indicates the number of caller-callee pairs. \texttt{CC} indicates the cyclomatic complexity of the caller and callee classes. \texttt{Calls} indicates the number of calls from the caller to the callee. \texttt{Coupled branches} indicates the number of coupled branches.}
	\label{tab:cling:projects}
	\input{papers/cling/tables/projects-table.tex}
\end{table*}

The subjects of our studies are five Java projects, namely \textit{Closure compiler}, \textit{Apache commons-lang}, \textit{Apache commons-math}, \textit{Mockito}, and \textit{Joda-Time}. These 
projects have been used in prior studies to assess the coverage and the effectiveness of unit-level test case generation \cite{ma2015grt, Panichella2018, just2014defects4j, Shamshiri2016}, program repair \cite{smith2015cure, martinez2016astor}, fault localization \cite{pearson2017evaluating, b2016learning}, and regression testing \cite{noor2015similarity, lu2016does}.

To sample the classes under test, we first extract pairs of \texttt{caller} and \texttt{callee} classes (i.e., pairs with interaction calls) in each project. Then, we remove pairs that contain trivial classes, i.e., classes where the caller and callee methods have no decision point (i.e., with cyclomatic complexity equal to one). This is because methods with no decision points can be covered with single method calls at the unit testing level.
Note that similar filtering based on code complexity has been used and recommended in the related literature~\cite{Campos2017, molina2018java, Panichella2018}. From the remaining pairs, we sampled 140 classes from the five projects in total. 
We performed the sampling to have classes with a broad range of complexity and coupling. 
The least and most  complex classes in the selected class pairs have one and 5,034 branching nodes, respectively. Also, the caller class of the least and most coupled class pairs contain one and 453 call sites to the callee class, respectively. 
The numbers of pairs selected from each project are reported in Table~\ref{tab:cling:projects}. Each pair of caller and callee classes represents a target for \cling.

Our replication package\footnote{\url{https://github.com/STAMP-project/Cling-application}} contains the list of class pairs sampled for our study, their detailed statistics (i.e., cyclomatic complexity and the number of interaction calls), and the project versions.

%In the process of selecting subjects for the evaluation, we make sure that we choose the most complex classes that testing them, either manually or automatically, is harder. In the same time, we choose the most coupled classes to make sure that \integration can be applied to them. The steps that we follow for subject selection are:
%\begin{inparaenum}[(i)]
%    \item we randomly choose one of the fixed versions of each project. \todo{add versions} 
%    \item we evaluate each of the class pairs, in each of these versions, as a potential subject. We calculate four metrics for these potential subjects: the Cyclic Complexity Number (CCN) of each of these two classes and the number of call couplings between them (call coupling from the caller to the callee and vice versa).
%    \item we remove class pairs which are not in the Pareto set \cite{} of these four objectives.
%    \item also, we remove the class pairs that both of them do not have any decision point in their methods. The remaining class pairs in the Pareto set are our subjects. Finally,
%    \item we indicate the caller and callee class. We select the class that has more call sites to the other one as the \textit{caller class} and choose the other one as the \textit{callee class}.
%\end{inparaenum}
%By following these steps, we selected 140 class pairs from 5 projects.


% Since RQ3 is about detecting real faults, we focus our subject selection only on the buggy classes. For each fault of \dfj, we select the modified classes, between the buggy and fixed version of the project, as the buggy classes. These buggy classes should be tested by \evosuite and \integration (as callee class).
% Hence, for selecting subjects for the evaluation of RQ3, we find the most complex class which has the most number of call sites to each of these modified classes. The reason that we choose the most complex classes is that these classes have more branching nodes in their CCFG. So, it provides more interesting edges.
% After this step, we have a set of class pairs for each of the faults: a modified class and its most complex caller. For each fault, we only choose one class pair that its caller has more call sites to the modified class.

% We set the modified class as the callee class and the other class as the caller class of the fault. Then, we sort the faults according to their caller and callee class: one fault is better than the other one if its caller class has more call sites to the callee class. Finally, we choose the best 50 faults according to this ranking. In this study, we limit the number of subjects for RQ3 to provide a good balance between the available amount of resources to run the experiment and generalisability of the results.


\subsubsection{Evaluation Procedure}
To answer the research questions, we run \cling on each of the selected class pairs. For each class pair targeted with \cling, we run \evosuite with the caller and the callee classes as target classes under test to compare the class integration test suite with unit level test suites for the individual classes. This results in having one integration-level test suite generated by \cling ($T_{\cling}$) and two unit-level test suites, one for the caller ($T_{R}$), and one for the callee ($T_{E}$). To address the random nature of evolutionary algorithms utilized by \cling and \evosuite, we repeat each run 20 times. Moreover, each \cling run is configured with a search budget of five minutes.  To allow a fair comparison, we run \evosuite for five minutes on each caller/callee class. Furthermore, \evosuite is configured to use the branch coverage criterion, using DynaMOSA as the search algorithm. 


For \textbf{RQ1}, we analyzed the average CBC coverages achieved by $T_{Cling}$ and compared them with the CBC coverages of $T_{R}$ across the 20 independent runs.


For \textbf{RQ2}, we measure the effectiveness of the generated test suite using \textit{mutation analysis} on the callee classes (considered as the class under test in our approach). Mutation analysis is a high-end coverage criterion, and mutants are often used as substitutes for real faults since previous studies highlighted its significant correlation with fault-detection capability~\cite{just2014mutants, andrews2005mutation}. Besides, mutation analysis provides a better measure of the test effectiveness compared to more traditional coverage criteria~\cite{wei2012branch} (e.g., branch coverage).

For the mutation analysis, we used PIT\footnote{http://pitest.org}, which is a state-of-the-art mutation testing tool for Java code, to mutate the callee classes. PIT has been used in literature to assess the effectiveness of test case generation tools~\cite{ma2015grt, panichella2017java, molina2018java, kifetew2019java}, and it has also been applied in industry\footnote{http://pitest.org/sky\_experience/}. In our study, we used PIT v. 1.4.9 with all mutation operators activated.

To answer \textbf{RQ2}, we compute the mutation scores achieved by $T_{\cling}$ for the callee class in each target class pair. Then, we compare it with the mutation scores achieved by $T_R$ and $T_E$ for the callee class. Moreover, we analyzed the orthogonality of the sets of mutants in the callee that were strongly killed by $T_{\cling}$, and those killed by the two unit-level tests ($T_{R}$ and $T_{E}$) individually. In other words, we look at whether $T_{\cling}$ allows killing mutants that are not killed at unit-level (strong mutation). Also, we analyze the type of the mutants which are only killed by $T_{\cling}$.


For \textbf{RQ3}, we analyzed the exceptions triggered by both integration and unit-level test suites. In particular, we extracted unexpected exceptions causing crashes, \ie exceptions that are triggered by the test suites but that are 
(i) not declared in the signature of the caller and callee methods using \texttt{throws} clauses, 
(ii) not caught by a \texttt{try-catch} blocks, and
(iii) not documented in the \texttt{Javadoc} of the caller or callee classes.  To answer \textbf{RQ3}, we then manually analyze unexpected exceptions that are triggered by the integration-level test cases (i.e., by \cling), but not by the unit-level tests. 

% For \textbf{RQ1}, we analyzed the average CBC coverage achieved by $T_{\cling}$, $T_{R}$, and $T_{E}$ across the \nrun independent runs.

% For \textbf{RQ2}, we measure the effectiveness of the generated test suite using \textit{mutation analysis} on the callee classes (only). Mutation analysis is a high-end coverage criterion, and mutants are often used as substitutes for real faults since previous studies highlighted its significant correlation with fault-detection capability~\cite{just2014mutants, andrews2005mutation}. Besides, mutation analysis provides a better measure of the test effectiveness compared to more traditional coverage criteria~\cite{wei2012branch} (e.g., branch coverage).

% For the mutation analysis, we used PIT\footnote{http://pitest.org}, which is a state-of-the-art mutation testing tool for Java code, to mutate the callee classes. PIT has been used in literature to assess the effectiveness of test case generation tools~\cite{ma2015grt,molina2018java}, and it has also been applied in industry\footnote{http://pitest.org/sky\_experience/}. In our study, we used PIT v. 1.4.9 with the all mutation operators activated.

% : \texttt{Conditionals Boundary}, \texttt{Increments}, \texttt{Invert Negatives}, \texttt{Math}, \texttt{Negate Conditionals}, \texttt{Return Values}, and \texttt{Void Method Calls} operators. 

% To answer \textbf{RQ2}, we compute the mutation scores achieved by $T_{\cling}$ for the callee class in each target class pair. Then, we compare it with the mutation scores achieved $T_R$ and $T_E$ for the callee class.
%Besides, it implements eight unit-level coverage criteria~\cite{rojas2015combining}: (1) \textit{branch}, (2) \textit{direct branch}, (3) \textit{line} (4) \textit{method}, (5) \textit{weak mutation}, (6) \textit{input}, (7) \textit{output}, and (8) \textit{exception coverage}. Prior studies showed that combining these multiple criteria can lead to test suite with better mutation score and fault detection capability~\cite{gay2017generating, Campos2018, Panichella2018a}. 

% For each class pair targeted with \cling, we ran \evosuite on both the caller and the callee separately. This results in having two unit-level test suites, one for the caller ($T_{R}$), and one for the callee ($T_{E}$). To answer \textbf{RQ2}, we analyzed the orthogonality of the sets of mutants in the callee that were strongly killed by the integration-level tests ($T_{\cling}$), and those killed by the two unit-level tests ($T_{R}$ and $T_{E}$) individually. In other words, we look at whether $T_{\cling}$ allows to kill mutants that are not killed at unit level (strong mutation). To allow a fair comparison, \evosuite was run for five minutes (the same search budget used for \cling) on each caller/callee class. Furthermore, \evosuite was configured to use the branch coverage criterion, using DynaMOSA as the search algorithm. 
%This allows us to address potential confounding factor due to the usage of different search algorithms and their implementation.
% To address the random nature of DynaMOSA, \evosuite was launched \nrun times on each class.

%To provide a deeper understanding of our results, we complement the analysis of the effectiveness (mutation analysis) by manually analyzing the mutants that are killed by the integration-level test suites but not by the unit-level suites. 

%\todo{To update if we do not use JaCoCo}
%We also used JaCoCo\footnote{https://www.jacoco.org} to compute line and branch coverage achieved by the different test suites. This analysis is performed to understand whether differences in test effectiveness are due to better/worst performance in classic unit-level coverage criteria.

%We run \evosuite for 10 minutes on each of the selected subjects (\textbf{\evosuite-10min}). Then, to examine the complementary impact of the generated tests by \integration, we dedicate half of the budget to \integration and run both of them on the selected subjects (\textbf{\evosuite-5min + \integration-5min}).
%Next, we run each of the generated tests for five times to find and remove the flaky tests.
 %To answer \textbf{RQ1} and \textbf{RQ2}, we compare line coverage , branch coverage (using JaCoCo\footnote{https://www.jacoco.org} which is a known code coverage tool), and mutation coverage (using PIT\footnote{http://pitest.org} as the most reliable mutation coverage tool for Java) between \emph{\evosuite-10min} and \emph{\evosuite-5min + \integration-5min}.



%\textit{Statistical analysis}. For the statistical analysis, we use the non-parametric Wilcoxon rank sum test \cite{conover1980} with a $p$-value threshold of 0.05. Besides, we use the Vargha-Delaney ($\hat{A}_{12}$) statistic \cite{vargha} to measure the magnitude of this difference between the performance achieved by \cling and \evosuite. In this study, $\hat{A}_{12} > 0.5$ shows that \cling outperforms \evosuite, while  $\hat{A}_{12} < 0.5$ indicates the opposite. \todo{Once the results are complete, we should replace the word ``performance'' with the actual metric we use in the comparison.}
%\medskip

\subsubsection{Flaky tests}

The test suites generated by \cling and \evosuite may contain flaky tests, \ie test cases that exhibit intermittent failures if executed with the same configuration. To detect and remove flaky tests, we ran each generated test suite five times. Then, we removed those tests that fail in at least one of the independent runs. Therefore, the test suites used to answer our three research questions likely do not contain flaky tests.

%To answer \textbf{RQ3}, we compared the captured exceptions by \cling generated tests with the captured exceptions by \emph{\evosuite-5min} and \emph{\evosuite-10min}. When the test generation process of \integration and \evosuite finds a bug in a generated test, it puts the exception throwing statement in a try/catch. To draw this comparison between \integration and \evosuite, we remove these try/catches from the flaky-free generated tests of \emph{\integration-5min}, \emph{\evosuite-5min}, and \emph{\evosuite-10min}. Then, we execute each of them on the project. Next, we filter out the stack traces thrown by \emph{\integration-5min} which is captured by at least one of executions of \emph{\evosuite-5min} or \emph{\evosuite-10min}. Hence, we are sure that the remaining captured stack traces by \emph{\integration-5min} is not captured by any execution of \evosuite in this experiment. We saved these remaining stack traces for further analysis.
 
\subsubsection{Infrastructure}
We have used a cluster (with 20 CPU-cores, 384 GB memory, and 482 GB hard drive) for our evaluation. We have executed \integration and \evosuite (against caller and callee class) on each of the 140 collected subjects. To address the random nature of the evaluated search approaches, we have repeated each execution 20 times. In total, we have performed 8,400 independent executions.


%\subsection{Data Analysis Procedure}
%To answer \textbf{RQ1}, we compare the line coverage and branch coverage scores of the generated test suites by \emph{\evosuite-5min + \integration-5min} and \emph{\evosuite-10min}. For this purpose, we use the non-parametric Wilcoxon test \cite{conover1980} with a p-value threshold of 0.05 to check if the difference between the scores of these two differently generated test suites is significant. Also, we use the Vargha-Delaney ($\hat{A}_{12}$) statistic \cite{vargha} to measure the magnitude of this difference between the coverage scores achieved by two approaches (effect size). In this study, $\hat{A}_{12} > 0.5$ shows that \emph{\evosuite-5min + \integration-5min} reaches a better line/branch coverage, while  $\hat{A}_{12} < 0.5$ indicates the opposite.


%We use the same statistical procedure to assess the significance and effect size in the mutation scores of \emph{\evosuite-5min + \integration-5min} and \emph{\evosuite-10min} to answer \textbf{RQ2}.


%For RQ3, we perform a manual analysis on each of the saved stack traces to check whether they are expected exceptions or not. For this purpose, we check the signature of the observed methods in the stack trace. If the signatures of methods do not indicate that these methods are throwing this type of exception, we count the crash as an unexpected crash which is only captured by \integration.