
\subsection{Empirical evaluation for crash reproduction}

Conducting empirical evaluation for crash reproduction is challenging. It requires to collect various artifacts from different sources and to analyze the results to determine, in the case of a negative outcome, the cause that prevents the crash reproduction. Some are easy to fix, like missing dependencies that were added to the project linked to the stack trace, and for which we rerun the evaluation on the stack traces. The others are detailed in Section \ref{sec:jcrashpack:challenges}, and serve to identify future research directions. 

One of the most surprising causes is due to a line mismatch in some stack traces.
During the manual analysis of our results, we found out that three frames in two different stack traces, coming from Defects4J projects, target the wrong lines in the source code: the line numbers in the stack traces point to lines in the source code that cannot throw the targeted exception.
Since the stack traces were collected directly from the Defects4J data (which reports failing tests and their outputs), we tried to regenerate them using the provided test suite and found a mismatch between the line numbers of the stack traces indeed.
We reported those two projects to the Defects4J developers:\footnote{See the issue at \url{https://github.com/rjust/defects4j/issues/142}.} a bug in JDK7~\cite{JDK7024096} causes this mismatch.
Since \evocrash relies on line numbers to guide its search, it could not reproduce the crashes.
We recompiled the source code, updated the stack trace accordingly in \crashpack, and rerun the evaluation for those two stack traces.

Thanks to \crashpack and \exrunner, we are now able to ease empirical evaluation for crash reproduction. \exrunner can be extended to other crash reproduction tools\footnote{See how to extend \exrunner at \url{https://github.com/STAMP-project/ExRunner}.} for comparison, or assess the development of new ideas in existing tools. Our future work also includes the prioritization of crashes from \crashpack to allow quick feedback on new ideas in a fast and automated way~\cite{Alshahwan2018}.


\subsection{Usefulness for debugging}


In our evaluation, we focused on the crash-replication capabilities of \evocrash and identified problems affecting those capabilities. 
We considered the generated tests only to classify the outcomes of the \evocrash generation process but did not assess their actual usefulness for debugging. 

Chen \etal \cite{Chen2015} introduced a usefulness criterion for the crash reproduction approaches. 
According to this criterion, a crash reproducing test is useful to the developers if it covers the buggy frame: i.e., if the target frame for which the reproduction is successful is higher than the frame that points to the buggy method.

In our previous work \cite{Soltani2018a}, we conducted a controlled experiment to assess the usefulness of \evocrash for debugging and bug fixing of two crashes (one from Apache Commons Collections and one from Apache Log4j) with 35 master students. Results show that using a crash-replicating test case generated by \evocrash may help to locate and fix the defects faster. Also, this study confirmed the usefulness criterion defined by the Chen \etal \cite{Chen2015} but also found evidence that test cases categorized as not useful can still help developers to fix the bug. 

Since \crashpack also includes two open source industrial and actively maintained applications, it represents an excellent opportunity to confirm the usefulness of \evocrash in an industrial setting. 
The key idea is to centralize the information in the issue tracker by providing a test case able to replicate the crash reported in an issue in the same issue (as an attachment for instance).
This can be automated using, for instance, a GitHub, GitLab or JIRA plugin that executes \evocrash when a new issue contains a stack trace.
To assess the usefulness of \evocrash in an industrial setting, we plan to setup a case study \cite{Wohlin2012} with our industrial partners. 
Hereafter, we outline the main steps of the evaluation protocol using XWiki as subject: 
\begin{inparaenum}[(i)]
\item select four crashes to fix (two from open issues and two from closed issues) for which \evocrash could generate a crash reproducing test for frame 3 or higher;
\item clone the XWiki Git repository in GitHub and open four issues, corresponding to the four crash;
\item remove the fix for the two fixed issues;
\item for each issue, append the test case generated by \evocrash; \label{item:addtestcases}
\item ask (non-XWiki) developers to fix the issues;
and finally, \item repeat the same steps without adding the test cases generated by \evocrash (i.e., omit step \ref{item:addtestcases}).
\end{inparaenum}
%
We would measure the time required to fix the issues (by asking participants to log that time). For the two previously fixed issues, we will compare the fixes provided by the participants with the fixes provided by XWiki developers. And for the two open issues, we will ask feedback from the XWiki developers through a pull request with the different solutions. 

%However, this evaluation is not trivial and requires a significant effort to either control and observe the environment of the developer when asked to perform a debugging task. 
%Or, to analyze and compare the generated crash-reproducing test with the messages in the issue tracker and the code fix in the code version system, with additional threats to the validity linked to the interpretation of the notes in the issue tracker and of the code fix. 

\subsection{Benchmark building}

\crashpack is the first benchmark dedicated to crash reproduction.
We deliberately made a biased selection when choosing Elasticsearch as the most popular, trending, and frequently-forked project from GitHub.
Elasticsearch was among several other highly ranked projects, which addressed other application domains, and thus were interesting to explore.
In the future, further effort should extend \crashpack, possibly by:
\begin{inparaenum}[(i)]
\item using a \textit{random selection} methodology for choosing projects; 
\item involving industrial projects from other application domains;
and \item automatically collecting additional information about the crashes, the stack traces, and the frames to further understand current strengths and limitations of crash reproduction.
\end{inparaenum}

Building \crashpack required substantial manual effort, not just for finding the issues, but also for collecting the right versions of the system itself and its dependencies needed  to reproduce the given crash.
Since we want it to be representative of current crashes, we need to automate this effort as much as possible: for instance, by mining stack traces from issue tracking systems \cite{Nayrolles2016}.

Despite the benefits that the evaluation infrastructure could get from the inclusion of \crashpack bugs in Defects4J, i.e., the isolation of the bugs to ease replicatbility of the evaluations \cite{just2014defects4j}, we designed \crashpack as a standalone instead of extending Defects4J.
The main reason is that not all bugs in Defects4J manifest as crashes (only 73 out of 395 where selected to be part of \crashpack). 
We also believe that the integration of the two benchmarks is not a smooth and easy process.
Defects4J requires isolation of the buggy and fixed versions of the source code, as wel as a test case able to expose the bug \cite{just2014defects4j}. However, not all issues were fixed at the time we collected the crashes in \crashpack.
Also, XWiki and Elasticsearch are much larger applications (124,000 NCSS for Elasticsearch, 177,000 NCSS for XWiki distributed in a hierarchy of several thousands of Maven projects) compared to the API libraries considered in Defects4J (63,000 NCSS for JFreeChart). 
Only building them with their default test suites already raised several issues. 
For those reasons, isolating the bug, the patch, and the non-regression test cases for such kind of large projects is not a trivial task. 







