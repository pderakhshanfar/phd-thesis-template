

% These techniques typically either use program \emph{runtime data} \cite{Artzi2008, Clause2007, Narayanasamy2005, Steven2000, Gomez2016, Bell2013, Cao2014, Rossler2013} or crash \emph{stack traces} \cite{BPT17concrash, soltani2017, Nayrolles2017, Xuan2015, Chen2015} to generate a test case that triggers the reported crash.  

% When available, runtime data offer more information to accurately reproduce a crash. However, it also raises various concerns (for instance, privacy violation) and may induce a significant overhead during data collection \cite{Chen2015, Nayrolles2017, Rossler2013}. Instead, we focus on crash reproduction based on a crash stack trace generated by a failing system. Practically, those stack traces are collected from the logs produced by the operating environment or reported by users in an issue tracking system. Various auromated crash stack trace-based reproduction approaches have been implemented and evaluated on different benchmarks \cite{soltani2017, Nayrolles2017, Xuan2015, Chen2015}. However, those benchmarks contains a limited number of crashes and associated stack traces.

A recent study presented a search-based approach called \evocrash, which applies a guided genetic algorithm to search for a crash reproducing test case \cite{soltani2017}, and demonstrated its relevance for debugging \cite{Soltani2018a}.
This study conducted an empirical evaluation on 54 crashes from commonly used utility libraries to compare \evocrash with state-of-the-art techniques for crash reproduction \cite{soltani2017}.
This was enough to show that the search-based crash reproduction outperformed other approaches based on backward symbolic execution \cite{Chen2015}, test case mutation \cite{Xuan2015}, and model-checking \cite{Nayrolles2017}, evaluated on smaller benchmarks.

However, all those crashes benchmarks were not selected to reflect challenges that are likely to occur in real life stack traces, raising threats to external validity.
Thus the questions whether the selected applications and crashes were sufficiently representative, if \evocrash will work in other contexts, and what limitations are still there to address, remained unanswered.

The goal of this chapter is to facilitate sound empirical evaluation on automated crash reproduction approaches.
To that end, we devise a new benchmark of real-world crashes, called \crashpack.
It contains 200 crashes from seven actively maintained open-source and industrial projects.
These projects vary in their domain application and include an enterprise wiki application, a distributed RESTful search engine, several popular APIs, and a mocking framework for unit testing Java programs. 
\crashpack is extensible, and can be used for large-scale evaluation and comparison of automated crash reproduction techniques for Java programs.

To illustrate the use of \crashpack, we adopt it to extend the reported evaluation on \evocrash~\cite{soltani2017} and identify the areas where the approach can be improved.
In this experience report, we provide an account of the cases that were successfully reproduced by \evocrash (87 crashes out of 200). 
We also analyze all failed reproductions and distill 14 categories of research and engineering limitations that negatively affected reproducing crashes in our study. 
Some of those limitations are in line with challenges commonly reported for search-based structural software testing in the community \cite{xiao2011precise, McMinn2011, Fraser2014b} and others are specific to search-based crash reproduction.

Our categorization of challenges indicates that environmental dependencies, code complexity, and limitations of automated input data generation often hinder successful crash reproduction.
In addition, stack frames (i.e., lines in a stack trace), pointing to varying types of program elements, such as interfaces, abstract classes, and anonymous objects, influence the extent to which a stack trace-based approach to crash reproduction is effective.

Finally, we observe that the percentage of successfully reproduced crashes drops from 85\% (46 crashes out of 54 reported by Soltani \etal \cite{Soltani2018a}) to 43\% (87 out of 200) when evaluating crashes that are from industrial projects. 
In our observations, generating input data for microservices, and unit testing for classes with environmental dependencies, which may frequently exist in enterprise applications, are among the major reasons for the observed drop in the reproduction rate.
These results are consistent with the paradigm shift to context-based software engineering research that has been proposed by Briand et al. \cite{Briand2017a}.

The key contributions of this chapter are:
\begin{itemize}
\item \crashpack,\footnote{Available at \url{https://github.com/STAMP-project/JCrashPack}.} a carefully composed benchmark of 200 crashes, as well as their correct system version and its libraries, from seven real-world Java projects, together with an account of our manual analysis on the characteristics of the selected crashes and their constituting frames, including size of the stack traces, complexity measures, and identification of buggy and fixed versions.

\item \exrunner,\footnote{Available at \url{https://github.com/STAMP-project/ExRunner-bash}} a bash script for automatically running experiments with crash reproduction tools in Java.

\item Empirical evidence,\footnote{A replication package for \evocrash results, their automated analysis, and the results of our manual analysis is available at \url{https://github.com/STAMP-project/EvoCrash-JCrashPack-application}.} demonstrating the effectiveness of search-based crash reproduction on real world crashes taken from \crashpack.

\item The identification of 14 categories of research and engineering challenges for search-based crash reproduction that need to be addressed in order to facilitate uptake in practice of crash reproduction research.

\end{itemize}
%\todoall{Please review the item for \exrunner!}

The remainder of the chapter is structured as follows:
Sections~\ref{sec:jcrashpack:benchmarkdesign} to \ref{sec:jcrashpack:ExRunner} describe the design protocol for the benchmark, the resulting benchmark \crashpack, as well as the \exrunner tool to run experiments on \crashpack.
Sections~\ref{sec:jcrashpack:evocrashevalsetup} to \ref{sec:jcrashpack:challenges} cover the experimental setup for the \evocrash evaluation, the results from our evaluation, and the results challenges that we identified through our evaluation.
Sections~\ref{sec:jcrashpack:discussion} to \ref{sec:jcrashpack:conclusion} provide a discussion of our results and future research directions, an analysis of the threats to validity, and a summary of our overall conclusions.

