This thesis answers the aforementioned research questions by following an approach based on design science \cite{Hevner2004}.
The design science contains two iterative phases: \textbf{Build} and \textbf{Evaluation}. The former phase concentrates on developing a purposeful artifact to solve an unsolved problem (here, search-based test generation techniques for specific behaviors such as crash reproduction and class integrations). The latter phase evaluates the designed artifact.
The \textit{Evaluation} phase reveals the limitations and challenges in the built artifact, and thereby, the weaknesses can be identified and resolved in the next phase of the \textit{Build} process. This iteration usually continues multiple times, and in each iteration, one (or more) novel techniques are introduced (to improve the existing artifact) and be assessed by the \textit{Evaluation} process. Also, both \textit{Build} and \textit{Evaluation} evolve in this process according to the new findings. 

In this thesis, we define a framework for search-based test case generation for crash reproduction. This framework includes a benchmark for crash reproduction approaches containing real-world and non-trivial crashes, an extensible platform for search-based crash reproducing test case generation, and accompanying guidelines for efficient usage of the platform in an industrial setting. This framework applies the existing search-based crash reproduction approach to real-world crashes and identifies the challenges (to answer $RQ_1$). We then improve the crash reproduction approach by addressing the identified challenges (to answer $RQ_2$). Finally, we go beyond search-based crash reproduction and introduce a novel approach for generating tests to cover other specific behaviors (\eg class integration testing). Accordingly, we extend the evaluation process to assess the new artifacts, as well (to address $RQ_3$).

\subsection{Search-based Test Case Generation Implementations}
\subsubsection{Search-based crash reproduction (\botsing)}
In this thesis, we present \botsing\footnote{\url{https://github.com/STAMP-project/botsing}}: an open-source, extendable search-based crash reproduction framework. \botsing implements search-based crash reproduction approaches introduced in previous studies~\cite{Rossler2013, Soltani2018a, Soltani2018b}. The tool takes as input a stack trace and software under test. Then, it starts a single-objective or multi-objective search process to generate a test reproducing the crash.

\botsing has been designed as an extendable framework for implementing new features and search algorithms for crash reproduction. For example, in Chapter 3 we perform a study on the impact of various \emph{seeding} strategies on crash reproduction, we have implemented multiple seeding strategies in \botsing.

From an industrial perspective, \botsing is used by our partners in the STAMP project.\footnote{Available at \url{http://stamp-project.eu/}}
They confirmed the relevance of \botsing for debugging and fixing application crashes~\cite{D57}.
The feedback ---as well as the crash reproducing test cases--- from our partners using \botsing are openly available in the STAMP GitHub repository.\footnote{Available at \url{https://github.com/STAMP-project/botsing-usecases-output}.}

\subsubsection{Search-based test generation for class integration (\cling)}
Chapter 6 addresses the $RQ_3$ by introducing a novel search-based technique to test the integration between two classes their call-sites information. We have implemented this approach as an open-source tool called \cling\footnote{\url{https://github.com/STAMP-project/botsing/tree/master/cling}}. This tool gets \emph{application's bytecode} and two classes in the application (\emph{caller class} and \emph{callee class}) and produces a test suite that covers the various interactions between these two classes.

\subsubsection{Common/uncommon execution patterns test generation in unit testing}
In Chapter 7, we implement novel secondary objectives considering the common/uncommon execution patterns in \evosuite, which is a well-known unit test generation framework.

\subsection{Evaluation infrutructure}

\subsubsection{Assessing crash reproduction}
To evaluate the different search-based crash reproduction techniques, we created \jcrashpack, an open-source crash benchmark, which contains 200 non trivial Java crashes collected from seven open-source projects: \textit{Closure compiler}, \textit{Apache commons-lang}, \textit{Apache commons-math}, \textit{Mockito}, \textit{Joda-Time}, \textit{XWiki}, and \textit{ElasticSearch}.
Moreover, to ease benchmarking using \crashpack, we developed a bash-based execution runner, openly available on GitHub.\footnote{\url{https://github.com/STAMP-project/ExRunner-bash}} This experiment runner (called \exrunner) runs different instances of a crash reproduction tool (here, \botsing) in parallel processes and collects relevant information about the execution in a CSV file. These collected data helps to identify the search-based crash reproduction benchmark.

\subsubsection{Assessing class integration} 
To assess \cling against the state-of-the-art, we used subjects from five Java projects, namely \textit{Closure compiler}, \textit{Apache commons-lang}, \textit{Apache commons-math}, \textit{Mockito}, and \textit{Joda-Time}. These 
projects have been used in prior studies to assess the coverage and the effectiveness of unit-level test case generation \cite{ma2015grt, Panichella2018, just2014defects4j, Shamshiri2016}, program repair \cite{smith2015cure, martinez2016astor}, fault localization \cite{pearson2017evaluating, b2016learning}, and regression testing \cite{noor2015similarity, lu2016does}.

Moreover, we have implemented another runner (similar to \exrunner). This runner collects more information about the test suites generated by different approaches: \textit{branch coverage} and mutatio score measured by PIT\footnote{http://pitest.org}, which is a state-of-the-art mutation testing tool for Java code, to mutate the callee classes.

\subsubsection{Assessing common/uncommon execution patterns test generation} 

To assess our common/uncommon execution patterns search objective, we choose \jabref (46 KLOC), an open-source Java bibliography reference manager with a graphical user interface working with BibTex files. We instrumented \jabref using Spoon~\cite{pawlak:hal-01169705} to monitor the execution paths while users are using it. We sampled 150 classes from this project for our evaluation.

Since we want to measure the strong mutation score of test suites generated by \evosuite + our novel secondary objectives against regular \evosuite, we use the same infrastructure as the one we used to assess \cling.